{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "068eb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Demucs model...\n",
      "Using device: cuda:0\n",
      "\n",
      "It's TIME!!!\n",
      "Press Ctrl + C or I + I to stop\n",
      "\n",
      "Stopping audio processing...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "from torchaudio.transforms import Fade\n",
    "from collections import deque\n",
    "\n",
    "# Input settings (CABLE Output)\n",
    "INPUT_SAMPLE_RATE = 44100\n",
    "INPUT_DEVICE = 34\n",
    "INPUT_CHANNELS = 2\n",
    "\n",
    "# Output settings (Headphones/Speakers)\n",
    "OUTPUT_SAMPLE_RATE = 48000\n",
    "OUTPUT_DEVICE = 31\n",
    "OUTPUT_CHANNELS = 2\n",
    "\n",
    "# Get BLOCK_SIZE samples out of 44100, and run inference on the last BLOCK_SIZE * WINDOW_BLOCKS\n",
    "DTYPE = np.float32\n",
    "BLOCK_SIZE = 4096\n",
    "WINDOW_BLOCKS = 4\n",
    "OVERLAP = 0.45\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self):\n",
    "        self.input_queue = queue.Queue()\n",
    "        self.output_queue = queue.Queue()\n",
    "        self.block_buffer = deque(maxlen=WINDOW_BLOCKS)\n",
    "        \n",
    "        print(\"Loading Demucs model...\")\n",
    "        self.model = HDEMUCS_HIGH_MUSDB.get_model()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=INPUT_SAMPLE_RATE,\n",
    "            new_freq=OUTPUT_SAMPLE_RATE,\n",
    "            dtype=torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        overlap_frames = int(BLOCK_SIZE * WINDOW_BLOCKS * OVERLAP)\n",
    "        self.fade = Fade(\n",
    "            fade_in_len=overlap_frames,\n",
    "            fade_out_len=overlap_frames,\n",
    "            fade_shape=\"linear\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.prev_chunk = None\n",
    "        \n",
    "        # Buffer should be silence before it's written\n",
    "        silence = np.zeros((BLOCK_SIZE, INPUT_CHANNELS), dtype=DTYPE)\n",
    "        for _ in range(WINDOW_BLOCKS - 1):\n",
    "            self.block_buffer.append(silence.copy())\n",
    "    \n",
    "    def input_callback(self, indata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Input status: {status}\")\n",
    "        self.input_queue.put(indata.copy())\n",
    "    \n",
    "    def output_callback(self, outdata, frames, time, status):\n",
    "        if status:\n",
    "            print(f\"Output status: {status}\")\n",
    "        try:\n",
    "            data = self.output_queue.get_nowait()\n",
    "            outdata[:] = data\n",
    "        except queue.Empty:\n",
    "            outdata.fill(0)\n",
    "    \n",
    "    def process_audio(self, audio_chunk):\n",
    "        try:\n",
    "            # Add new chunk to buffer and concatenate into a bigger chunk\n",
    "            self.block_buffer.append(audio_chunk)\n",
    "            combined_chunk = np.concatenate(list(self.block_buffer), axis=0)\n",
    "\n",
    "            # Convert to torch tensor, normalize and move to device\n",
    "            waveform = torch.from_numpy(combined_chunk).to(self.device).T.unsqueeze(0)\n",
    "            ref = waveform.mean(0)\n",
    "            waveform = (waveform - ref.mean()) / (ref.std() + 1e-7)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                # GPU consumption can probably be approximated by:\n",
    "                # %time below and divide wall time by BLOCK_SIZE/INPUT_SAMPLE_RATE\n",
    "                sources = self.model.forward(waveform)\n",
    "                # GPU consumption can be limited by adjusting block_size by consumption\n",
    "\n",
    "            # Get strike sounds, denormalize, and fade\n",
    "            drums = self.fade(sources[:, 0] * ref.std() + ref.mean())\n",
    "\n",
    "            if self.prev_chunk is not None:\n",
    "                overlap_size = int(BLOCK_SIZE * WINDOW_BLOCKS * OVERLAP)\n",
    "                drums[:, :, :overlap_size] += self.prev_chunk[:, :, -overlap_size:]\n",
    "            self.prev_chunk = drums.clone()\n",
    "\n",
    "            # Different frequencies so gotta resample\n",
    "            drums = self.resampler(drums)\n",
    "\n",
    "            # This is the key change to fix the choppy output\n",
    "            drums_numpy = drums.cpu().squeeze(0).permute(1, 0).numpy()\n",
    "\n",
    "            # Crossfade a bit of the last block, and leave some for the next block\n",
    "            if hasattr(self, 'previous_output_remainder'):\n",
    "                crossfade_size = int(BLOCK_SIZE * 0.5)  # 50% overlap for crossfade\n",
    "                fade_in = np.linspace(0, 1, crossfade_size)\n",
    "                fade_out = np.linspace(1, 0, crossfade_size)\n",
    "\n",
    "                # Apply crossfade\n",
    "                current_block = np.zeros((BLOCK_SIZE, OUTPUT_CHANNELS))\n",
    "                current_block[:crossfade_size] = self.previous_output_remainder[-crossfade_size:] * fade_out[:, np.newaxis]\n",
    "                current_block[:crossfade_size] += drums_numpy[:crossfade_size] * fade_in[:, np.newaxis]\n",
    "                current_block[crossfade_size:] = drums_numpy[crossfade_size:BLOCK_SIZE]\n",
    "\n",
    "                # Save the remainder for next time\n",
    "                self.previous_output_remainder = drums_numpy[:BLOCK_SIZE*2]\n",
    "            else:\n",
    "                # First run, no previous output to crossfade with\n",
    "                current_block = drums_numpy[:BLOCK_SIZE]\n",
    "                self.previous_output_remainder = drums_numpy[:BLOCK_SIZE*2]\n",
    "\n",
    "            return current_block\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_audio: {e}\")\n",
    "            return np.zeros((BLOCK_SIZE, OUTPUT_CHANNELS))\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"\\nIt's TIME!!!\")\n",
    "        print(\"Press Ctrl + C or I + I to stop\")\n",
    "        \n",
    "        input_stream = None\n",
    "        output_stream = None\n",
    "        \n",
    "        try:\n",
    "            input_stream = sd.InputStream(\n",
    "                device=INPUT_DEVICE,\n",
    "                channels=INPUT_CHANNELS,\n",
    "                samplerate=INPUT_SAMPLE_RATE,\n",
    "                dtype=DTYPE,\n",
    "                blocksize=BLOCK_SIZE,\n",
    "                callback=self.input_callback\n",
    "            )\n",
    "            \n",
    "            output_stream = sd.OutputStream(\n",
    "                device=OUTPUT_DEVICE,\n",
    "                channels=OUTPUT_CHANNELS,\n",
    "                samplerate=OUTPUT_SAMPLE_RATE,\n",
    "                dtype=DTYPE,\n",
    "                blocksize=BLOCK_SIZE,\n",
    "                callback=self.output_callback\n",
    "            )\n",
    "            \n",
    "            with input_stream, output_stream:\n",
    "                input_stream.start()\n",
    "                output_stream.start()\n",
    "                \n",
    "                while True:\n",
    "                    # Input -> Process -> Validate -> Play\n",
    "                    audio_chunk = self.input_queue.get()\n",
    "                    drums = self.process_audio(audio_chunk)\n",
    "                    # Output needs to be a specific shape or u get error\n",
    "                    if drums.shape != (BLOCK_SIZE, OUTPUT_CHANNELS):\n",
    "                        drums = np.resize(drums, (BLOCK_SIZE, OUTPUT_CHANNELS))\n",
    "                    self.output_queue.put(drums)\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping audio processing...\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "        finally:\n",
    "            if input_stream is not None and input_stream.active:\n",
    "                input_stream.stop()\n",
    "            if output_stream is not None and output_stream.active:\n",
    "                output_stream.stop()\n",
    "\n",
    "processor = AudioProcessor()\n",
    "processor.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
